import requests
import random

from arachnid.timewidgets import Timer
from arachnid.crawler.config import CrawlerConfig, generate_crawler_config
from arachnid.crawler import responseparser
from arachnid.crawler.scheduler import Scheduler, FuzzingOptions
from arachnid.crawler.scraper import Scraper
from arachnid.crawler.domaindata import DomainData
from arachnid.crawler.crawler_url import CrawlerURL
from arachnid.crawler import url_functions
from arachnid.crawler import warning_issuer


class Crawler:
    def __init__(self, seed, configuration=CrawlerConfig()):
        seed = CrawlerURL(seed)
        self.config = configuration
        self.headers = {"User-Agent": self.config.agent}
        fuzzing_options = FuzzingOptions(self.config.paths_list_file_loc if self.config.fuzz_paths else None,
                                         self.config.subs_list_file_loc if self.config.fuzz_subs else None)
        self.schedule = Scheduler(seed,
                                  useragent=self.config.agent,
                                  fuzzing_options=fuzzing_options,
                                  respect_robots=self.config.obey_robots,
                                  allow_subdomains=self.config.scrape_subdomains,
                                  blacklist_dirs=self.config.blacklisted_directories)
        self.output = DomainData(seed.get_netloc())
        self.output.start()
        self.output.add_config(self.config)
        self.delay_sw = Timer()
        self._update_crawl_delay()
        self.delay_sw.start()

    def crawl_next(self):
        c_url = self.schedule.next_url()
        if c_url is None:
            self.finish()
            return False
        print(c_url)
        self.delay_sw.wait()
        try:
            self.parse_c_url_page_content(c_url)
        except BaseException as e:
            warning_issuer.issue_warning_from_exception(e, c_url.get_url())
            self.schedule.report_found_urls([])
        self._update_crawl_delay()
        self.delay_sw.start()
        return True

    def parse_c_url_page_content(self, c_url):
        r = requests.get(c_url.get_url(), headers=self.headers, timeout=30)
        warning_issuer.issue_warning_from_status_code(r.status_code, c_url.get_url())
        if "content-type" in r.headers.keys():
            if "text/html" in r.headers["content-type"]:
                self._parse_page(r, c_url)
            else:
                self._parse_document(r, c_url)

    def finish(self):
        self.output.end()

    def _parse_page(self, response, c_url):
        """ Parses the page and sends information to output. Process include (according to configuration)
            - Gathering emails, phone numbers, social media, custom_regex
            - Reporting newly discovered links to the scheduler

            response is a response object generated by requests library
            c_url is a CrawlerURL object
        """
        scraper = Scraper(response.text, "html.parser")
        url_parts = c_url.get_url_parts()
        if self.config.scrape_email:
            for email in scraper.find_all_emails():
                self.output.add_email(email)
        if self.config.scrape_phone_number:
            for number in scraper.find_all_phones():
                self.output.add_phone(number)
        if self.config.scrape_social_media:
            for social in scraper.find_all_social():
                self.output.add_social(social)
        if self.config.custom_regex:
            for regex in scraper.find_all_regex(self.config.custom_regex):
                self.output.add_custom_regex(regex)
        found_c_urls = []
        if self.config.scrape_links:
            for page in scraper.find_all_http_refs():
                page = page.strip().replace(" ", "%20")
                url = url_functions.join_url(c_url.get_url(), page)
                found_c_urls.append(CrawlerURL(url, allow_query=self.config.allow_query))
        self.schedule.report_found_urls(found_c_urls)

        page_info = {"path": c_url.get_extension(),
                     "title": scraper.title.string if scraper.title and scraper.title.string else url_parts.path.split("/")[-1],
                     "custom_string_occurances": scraper.string_occurances(self.config.custom_str, self.config.custom_str_case_sensitive) if self.config.custom_str else None,
                     "on_fuzz_list": c_url.is_fuzzed(),
                     "on_robots": c_url.in_robots(),
                     "code": response.status_code}
        self.output.add_page(c_url.get_netloc(), page_info)

    def _parse_document(self, response, c_url):
        parser = responseparser.DocumentResponse(response, self.config.documents)
        data = parser.extract()
        self.schedule.report_found_urls([])
        if data:
            data["path"] = c_url.get_url_parts().path
            self.output.add_document(c_url.get_netloc(), data)

    def _update_crawl_delay(self):
        default_delay = random.choice(self.config.default_delay)
        s_delay = self.schedule.get_crawl_delay()
        self.delay_sw = Timer(default_delay if default_delay > s_delay else s_delay)

    def dumps(self, **kwargs):
        return self.output.dumps(**kwargs)


def get_crawler_from_namespace(namespace):
    return Crawler(namespace.seed, configuration=generate_crawler_config(namespace))
